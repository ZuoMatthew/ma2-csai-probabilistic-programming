\section{PGM to CNF}

\subsection{ENC 1}
Our ENC1 encoding for the Cancer Bayesian network can be found in appendix~\ref{ENC1}.

\subsection{ENC 2}
Our ENC2 encoding for the Cancer Bayesian network can be found in appendix~\ref{ENC2}.



\section{SRL to CNF}
\subsection{Encoding of Monty Hall as CNF}
An encoding of problog programs can be generated by our program as follows:
\begin{lstlisting}
python3 scripts/inference.py --problog files/problog/monty_hall.pl
\end{lstlisting}
The CNF will be shown using the program's predicates. A version of the CNF in dimacs format will be shown as well.
See \texttt{README.MD} for more information.

Our CNF encoding for the given Monty Hall ProbLog program is:
\begin{align*}
    \land & (open\_door(2) \lor prize(2) \lor prize(3) \lor \neg p\_open\_door(2)\_0) \\
    \land & (open\_door(2) \lor prize(2) \lor \neg prize(3))                          \\
    \land & (\neg open\_door(2) \lor \neg prize(2) \lor \neg prize(2))                \\
    \land & (\neg open\_door(2) \lor \neg prize(2) \lor prize(3))                     \\
    \land & (\neg open\_door(2) \lor \neg prize(3) \lor \neg prize(2))                \\
    \land & (\neg open\_door(2) \lor \neg prize(3) \lor prize(3))                     \\
    \land & (\neg open\_door(2) \lor p\_open\_door(2)\_0 \lor \neg prize(2))          \\
    \land & (\neg open\_door(2) \lor p\_open\_door(2)\_0 \lor prize(3))               \\
    \land & (open\_door(3) \lor prize(2) \lor prize(3) \lor \neg p\_open\_door(3)\_0) \\
    \land & (open\_door(3) \lor prize(3) \lor \neg prize(2))                          \\
    \land & (\neg open\_door(3) \lor \neg prize(2) \lor \neg prize(3))                \\
    \land & (\neg open\_door(3) \lor \neg prize(2) \lor prize(2))                     \\
    \land & (\neg open\_door(3) \lor \neg prize(3) \lor \neg prize(3))                \\
    \land & (\neg open\_door(3) \lor \neg prize(3) \lor prize(2))                     \\
    \land & (\neg open\_door(3) \lor p\_open\_door(3)\_0 \lor \neg prize(3))          \\
    \land & (\neg open\_door(3) \lor p\_open\_door(3)\_0 \lor prize(2))               \\
    \land & (win\_keep \lor \neg prize(1))                                            \\
    \land & (\neg win\_keep \lor prize(1))                                            \\
    \land & (win\_switch \lor \neg prize(2) \lor open\_door(2))                       \\
    \land & (win\_switch \lor \neg prize(3) \lor open\_door(3))                       \\
    \land & (\neg win\_switch \lor prize(2) \lor prize(3))                            \\
    \land & (\neg win\_switch \lor prize(2) \lor \neg open\_door(3))                  \\
    \land & (\neg win\_switch \lor \neg open\_door(2) \lor prize(3))                  \\
    \land & (\neg win\_switch \lor \neg open\_door(2) \lor \neg open\_door(3))        \\
    \land & (\neg prize(1) \lor \neg prize(2))                                        \\
    \land & (\neg prize(1) \lor \neg prize(3))                                        \\
    \land & (\neg prize(2) \lor \neg prize(3))                                        \\
    \land & (prize(1) \lor prize(2) \lor prize(3))\\
    & \textbf{Weights:}             \\
    & W(p\_open\_door(2)\_0) = 0.5 & W &(\neg p\_open\_door(2)\_0) = 0.5 \\
    & W(p\_open\_door(3)\_0) = 0.5 & W &(\neg p\_open\_door(3)\_0) = 0.5 \\
    & W(select\_door(1)) = 1.00    & W &(\neg select\_door(1)) = 0.00    \\
    & W(prize(1)) = 0.33           & W &(\neg prize(1)) = 1.00           \\
    & W(prize(2)) = 0.33           & W &(\neg prize(2)) = 1.00           \\
    & W(prize(3)) = 0.33           & W &(\neg prize(3)) = 1.00           \\
    & W(open\_door(2)) = 1.00      & W &(\neg open\_door(2)) = 1.00      \\
    & W(open\_door(3)) = 1.00      & W &(\neg open\_door(3)) = 1.00      \\
    & W(win\_keep) = 1.00          & W &(\neg win\_keep) = 1.00          \\
    & W(win\_switch) = 1.00        & W &(\neg win\_switch) = 1.00        \\
\end{align*}


\section{Weighted Model Counting}
\subsection{Weighted model counters on above CNFs}
We have selected MiniC2D and Cachet as weighted model counters.

The output of the model counters can be found in the listings below.

\subsubsection{MiniC2D}
MiniC2D needs to be executed with the $-W$ flag in order for it to do weighted model counting.
The resulting probability can be read next to ``Count''.

\begin{lstlisting}[caption={MiniC2D on ENC1 encoding of Cancer network}]
Constructing CNF... DONE
CNF stats:
  Vars=30 / Clauses=74
  CNF Time	0.000s
Constructing vtree (from primal graph)... DONE
Vtree stats:
  Vtree widths: con<=5, c_con=48 v_con=5
  Vtree Time	0.001s
Counting... DONE
  Learned clauses      	0
Cache stats:
  hit rate   	75.0%
  lookups    	16
  ent count  	4
  ent memory 	0.2 KB
  ht  memory 	152.6 MB
  clists     	1.0 ave, 1 max
  keys       	3.0b ave, 3.0b max, 3.0b min
Count stats:
  Count Time	0.000s
  Count 	0.9999999999999999
Total Time: 0.012s
\end{lstlisting}

\begin{lstlisting}[caption={MiniC2D on ENC2 encoding of Cancer network}]
Constructing CNF... DONE
CNF stats:
  Vars=20 / Clauses=30
  CNF Time	0.000s
Constructing vtree (from primal graph)... DONE
Vtree stats:
  Vtree widths: con<=6, c_con=16 v_con=6
  Vtree Time	0.000s
Counting... DONE
  Learned clauses      	0
Cache stats:
  hit rate   	23.1%
  lookups    	26
  ent count  	20
  ent memory 	1.0 KB
  ht  memory 	152.6 MB
  clists     	1.0 ave, 1 max
  keys       	1.8b ave, 3.0b max, 1.0b min
Count stats:
  Count Time	0.000s
  Count 	1.0000000000000000
Total Time: 0.012s
\end{lstlisting}

\begin{lstlisting}[caption={MiniC2D on WCNF encoding of Monty Hall}]
Constructing CNF... DONE
CNF stats:
  Vars=10 / Clauses=26
  CNF Time	0.000s
Constructing vtree (from primal graph)... DONE
Vtree stats:
  Vtree widths: con<=4, c_con=22 v_con=4
  Vtree Time	0.000s
Counting... DONE
  Learned clauses      	0
Cache stats:
  hit rate   	20.0%
  lookups    	5
  ent count  	4
  ent memory 	0.2 KB
  ht  memory 	152.6 MB
  clists     	1.0 ave, 1 max
  keys       	3.2b ave, 4.0b max, 3.0b min
Count stats:
  Count Time	0.000s
  Count 	1.0000000000000000
Total Time: 0.011s
\end{lstlisting}

\subsubsection{Cachet}
For Cachet, there is no need to use extra parameters to get a probability.
It is reported next to ``Satisfying probability''.

\begin{lstlisting}[caption={Cachet on ENC1 encoding of Cancer network}]
    Number of total components		11
Number of split components		2
Number of non-split components		5
Number of SAT residual formula		12
Number of trivial components		0
Number of changed components		0
Number of adjusted components		0
First component split level		1

Number of Decisions			11
Max Decision Level			5
Number of Variables			30
Original Num Clauses			74
Original Num Literals			172
Added Conflict Clauses			0
Added Conflict Literals			0
Deleted Unrelevant clauses		0
Deleted Unrelevant literals		0
Number of Implications			124
Total Run Time				0.0163

Satisfying probability			8.72319e-08
Number of solutions			93.6645

\end{lstlisting}

\begin{lstlisting}[caption={Cachet on ENC2 encoding of Cancer network}]
    Number of total components		11
Number of split components		2
Number of non-split components		5
Number of SAT residual formula		12
Number of trivial components		0
Number of changed components		0
Number of adjusted components		0
First component split level		1

Number of Decisions			11
Max Decision Level			5
Number of Variables			20
Original Num Clauses			30
Original Num Literals			84
Added Conflict Clauses			0
Added Conflict Literals			0
Deleted Unrelevant clauses		0
Deleted Unrelevant literals		0
Number of Implications			72
Total Run Time				0.017372

Satisfying probability			1
Number of solutions			1.04858e+06

\end{lstlisting}

\begin{lstlisting}[caption={Cachet on WCNF encoding of Monty Hall}]
Number of total components		4
Number of split components		1
Number of non-split components		2
Number of SAT residual formula		5
Number of trivial components		0
Number of changed components		0
Number of adjusted components		0
First component split level		2

Number of Decisions			4
Max Decision Level			4
Number of Variables			10
Original Num Clauses			26
Original Num Literals			73
Added Conflict Clauses			0
Added Conflict Literals			0
Deleted Unrelevant clauses		0
Deleted Unrelevant literals		0
Number of Implications			26
Total Run Time				0.016062

Satisfying probability			0.444444
Number of solutions			455.111
\end{lstlisting}

For ENC1 we see that with Cachet we get a satisfying probability of almost $0$. With Monty hall we can also see that we get a probability of 0.4. This is due to the fact that with ENC1 all our negative literals have a weight of 1, while Cachet expects that a literal $+$ its negation $= 1$. For Monty hall we also have negative literals with weight $1$ which gives the same problem as with ENC1.

\subsection{Difference between the selected WMCs}
%The differences between the different WMCs come from \cite{CHAVIRA2008772}.
%\subsubsection{Cachet vs jointree and recursive conditioning}
%Jointree and recursive conditioning only exploit topological structure thus they take no advantage of the massive determinism available in networks whilst Cachet does this.

\subsubsection{MiniC2D Vs Cachet}
MiniC2D and Cachet are weighted model counters that work in different ways. In short, MiniC2D is a top down compiler that compiles CNFs into SDDs, while Cachet uses formula caching combined clause learning and component analysis [\cite{MiniC2D},\cite{Cachet}]. MiniC2D's compilation to SDDs is faster and uses less memory.

Both weighted model counters use concepts from the SAT literature. They both use clause learning and component caching in order to reuse components that later appear again during search. \\
Cachet also uses other methods from SAT literature, like an explicit on the fly calculation of connected components. This is different in MiniC2D, as relies on vtrees to identify disconnected CNF components. MiniC2d creates vtrees for CNFs and then creates SDDs based on the created vtrees.
%The way that the compilation is done with MiniC2D is as follows:
%The SDD compilation is driven by the vtree, it uses this to identify disconnected CNF components and it uses a component caching scheme to prevent compiling the same component multiple times.


\subsection{Overview of computational requirements}
All the tests can be found in the test folder.
We used our scripts to create the dimac files. The input files for our enc1 and enc2 converter ard ``.dsc'' files which can be found at 
\url{http://www.bnlearn.com/bnrepository/discrete-small.html#cancer}.

\subsubsection{Test 1: Cancer network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 0.2 KB    & 0.155s   & 1.0    & 1.0 KB    & 0.000s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.016s       & 1.0     & ?    & 0.016s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 2: asia network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 0.9 KB    & 0.145s   & 1.0    & 2.0 KB    & 	 0.139s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.018s       & 1     & ?    & 0.017s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 3: sachs network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 0.99707  & 14.3 KB    & 0.184s   & 1.0    & 14.5 KB   & 	0.154s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.019s       & 1     & ?    & 0.017s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 4: earthquake network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 0.6 KB & 0.137s   & 1.0    & 1.0 KB  & 	0.153s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.016s       & 1     & ?    & 0.017s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 5: survey network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 1.6 KB    & 0.125s   & 1.0    & 2.0 KB    & 	0.125s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.016s       & 1     & ?    & 0.016s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 6: alarm network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 0.999  & 819.4 KB    & 0.215s   & 0.999    & 147.2 KB    & 	0.092s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.176s       & 1     & ?    & 0.222s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 7: andes network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 5.1GB    & 246.998s   & 1.0    & 364.1 MB    & 	12.06s \\
  \hline
\textbf{Cachet}  & ?  & ?    & $> 4h (killed)$      & b     & val3    & ?    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 8: child network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 45.8KB    & 0.076   & 1.0    & 30.8 KB    & 	0.059s \\
  \hline
\textbf{Cachet}  & 0  & ?    & 0.03s      & 1     & ?    & 0.03s    \\ \cline{1-7} 
\end{tabular}
\end{table}

\subsubsection{Test 9: hailfinder network}
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{c|c|c|c|c|c|c|}
\cline{2-7}
        & \multicolumn{3}{c|}{ENC1} & \multicolumn{3}{c|}{ENC2} \\ \cline{2-7} 
  & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} & \textbf{Prob}  & \textbf{Memory}  & \textbf{Runtime} \\ \cline{1-7} 
  \textbf{Minic2d} & 1.0  & 129.8 MB & 0.999   & 1.0    & 13.1 MB    & 	1.854 \\
  \hline
\textbf{Cachet}  & val1  & val2    & a       & b     & val3    & val4    \\ \cline{1-7} 
\end{tabular}
\end{table}


\section{Knowledge compilation}
\subsubsection{Vtree with the most compact circuit}
During our tests 
\subsubsection{Pattern for a good vtree}
As a vtree is a binary tree, which means that a good vtree is compact. We want thus a vtree that is shallow. 
